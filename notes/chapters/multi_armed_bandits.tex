\section{k-armed Bandit Problem}
With every step you take one of the k actions and get a reward.
The goal is to maximize the reward in the long run.
We attempt to figure out what the value function for a given action is.
EXPLOITATION vs. EXPLORATION

\section{Action-value Methods}
Action-value methods are methods that help assess the value of taking a given action.
\newline e-greedy - making the greedy choice most of the time and every so often,
    with probability e, making a random choice.

\section{The 10-armed Testbed}
    Example approach to solving the 10-armed bandit problem using a greedy approach and
    different e-greedy approaches.
    The higher the variance the better the e-greedy methods perform.
    If the variance of the reward distribution is low, or 0, the more greedy the approach the better.

\section{Incremental Implementation}
We define the incremental update formula for Qn as
\begin{myequation}{2.3}
    Q_{n+1} = Q_n + \frac{1}{n}\left[R_n - Q_n\right]
\end{myequation}

\begin{itemize*}
    \item $Qn$: estimate of the value from action \textit{a} given n-1 experienced steps.
    \item $Rn$ - reward received after nth selection of action a.
\end{itemize*}

\[NewEstimate = OldEstimate + StepSize * (Target - OldEstimate)\]
- this is the very generic rule that can be observed in many RL algorithms.

\section{Tracking a Nonstationary Problem}
This approach is when dealing with rewards that are not constant but change over time.
The \hyperref[eq:2.3]{equation} from the incremental implentation is modified to be
\begin{myequation}{2.5}
    Q_{n+1}\doteq Q_n + \alpha[R_n - Q_n]
\end{myequation}
\begin{itemize*}
    \item $\alpha \in (0, 1]$: constant step size parameter.
\end{itemize*}
Now $Q_{n+1}$ can be represented as a weighted average of $Q_1$:
\begin{myequation}{2.6}
    Q_{n+1} = (1-\alpha)^nQ_1 + \alpha \sum_{i=1}^\infty(1-\alpha)^{n-i}R_i
\end{myequation}
The weight of $R_i$ declines the further in the past it is: $(1-\alpha)^{n-i}$.

\section{Optimistic Initial Values}
The initial values of $Q$ largely influence the algorithms behavior and can help
sway it in one or the other direction.
For example, if when using the action-value averages we were to be overly optimistic and set
$Q$'s initial values to +5 (when the actual values are lower than that) the algorithm will
do a lot of exploration in the beginnning, not being satisfied with the rewards it was receiving
after picking actions.
This exploration happens even if $\varepsilon$ is set to 0 and the algorithm is greedy.

\section{Upper-Confidence-Bound (UCB) Action Selection}
Attempting to pick exploratory actions based on their potential to be more optimal than the
current greedy option.
One effective way of doing this is:
\begin{myequation}{2.10}
    A_t = \argmax_a \left\lceil Q_t(a) + c \sqrt{ \frac{\ln{t}}{N_t(a)}} \ \right\rceil
\end{myequation}
\begin{itemize*}
    \item $N_t$: the number of times action $a$ has been picked prior to timestep $t$.
    \item $\ln{t}$: the natural algorithm of $t$.
    \item $c$: the degree of exploration.
\end{itemize*}
The square root expression represents the level of uncertainty or variance in the estimate
for action $a$.

\section{Gradient Bandit Algorithms}
Calculating a numerical preference for action $a$ denoted with $H_t(a)$.
The probabilities only depend on other actions, not rewards.
\begin{myequation}{2.11}
    Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)
\end{myequation}
\begin{itemize*}
    \item $\pi_t(a)$: the probability of taking action $a$ at timestep $t$.
    \item $\forall a, H_1(a) = 0$.
    \item $\alpha > 0$: step size parameter.
\end{itemize*}
After every step the preferences are updated the following way:
\begin{myequation}{2.12}
    \begin{aligned}
        &H_{t+1}(a) \doteq H_t(a) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(a)), &a = A_t \\
        &H_{t+1}(a) \doteq H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a),\ &a\neq A_t
    \end{aligned}
\end{myequation}
\begin{itemize*}
    \item $\bar{R}_t$: the average of all the rewards up to and including timestep $t$.
\end{itemize*}
The $\bar{R}_t$ is used as a baseline for the current $R$. If the current reward is higher than
the baseline then the probability increases and vice versa. The probabilities for the
non-selected actions move in the opposite direction.

\section{Associative Search (Contextual Bandits)}
In this case the optimal action is based on the state the environment is in.
Therefore, we learn a policy which actively maps state to optimal action.


\section{Summary}
\label{sec:mab-summary}
We have presented in this chapter several simple ways of balancing exploration and
exploitation. The "-greedy methods choose randomly a small fraction of the time, whereas
UCB methods choose deterministically but achieve exploration by subtly favoring at each
step the actions that have so far received fewer samples. Gradient bandit algorithms
estimate not action values, but action preferences, and favor the more preferred actions
in a graded, probabilistic manner using a soft-max distribution. The simple expedient of
initializing estimates optimistically causes even greedy methods to explore significantly.
It is natural to ask which of these methods is best. Although this is a difficult question
to answer in general, we can certainly run them all on the 10-armed testbed that we
have used throughout this chapter and compare their performances. A complication is
that they all have a parameter; to get a meaningful comparison we have to consider
their performance as a function of their parameter. Our graphs so far have shown the
course of learning over time for each algorithm and parameter setting, to produce a
learning curve for that algorithm and parameter setting. If we plotted learning curves
for all algorithms and all parameter settings, then the graph would be too complex and
crowded to make clear comparisons. Instead we summarize a complete learning curve
by its average value over the 1000 steps; this value is proportional to the area under the
learning curve. Figure 2.6 shows this measure for the various bandit algorithms from
this chapter, each as a function of its own parameter shown on a single scale on the
x-axis. This kind of graph is called a parameter study. Note that the parameter values
are varied by factors of two and presented on a log scale. Note also the characteristic
inverted-U shapes of each algorithm’s performance; all the algorithms perform best at
an intermediate value of their parameter, neither too large nor too small. In assessing
a method, we should attend not just to how well it does at its best parameter setting,
but also to how sensitive it is to its parameter value. All of these algorithms are fairly
insensitive, performing well over a range of parameter values varying by about an order
of magnitude. Overall, on this problem, UCB seems to perform best.
Despite their simplicity, in our opinion the methods presented in this chapter can
fairly be considered the state of the art. There are more sophisticated methods, but their
complexity and assumptions make them impractical for the full reinforcement learning
problem that is our real focus. Starting in Chapter 5 we present learning methods for
solving the full reinforcement learning problem that use in part the simple methods
explored in this chapter.
Although the simple methods explored in this chapter may be the best we can do
at present, they are far from a fully satisfactory solution to the problem of balancing
exploration and exploitation.
One well-studied approach to balancing exploration and exploitation in k -armed bandit
problems is to compute a special kind of action value called a Gittins index. In certain
important special cases, this computation is tractable and leads directly to optimal
solutions, although it does require complete knowledge of the prior distribution of possible
problems, which we generally assume is not available. In addition, neither the theory
nor the computational tractability of this approach appear to generalize to the full
reinforcement learning problem that we consider in the rest of the book.
The Gittins-index approach is an instance of Bayesian methods, which assume a known
initial distribution over the action values and then update the distribution exactly after
each step (assuming that the true action values are stationary). In general, the update
computations can be very complex, but for certain special distributions (called conjugate
priors) they are easy. One possibility is to then select actions at each step according
to their posterior probability of being the best action. This method, sometimes called
posterior sampling or Thompson sampling, often performs similarly to the best of the
distribution-free methods we have presented in this chapter.
In the Bayesian setting it is even conceivable to compute the optimal balance between
exploration and exploitation. One can compute for any possible action the probability
of each possible immediate reward and the resultant posterior distributions over action
values. This evolving distribution becomes the information state of the problem. Given
a horizon, say of 1000 steps, one can consider all possible actions, all possible resulting
rewards, all possible next actions, all next rewards, and so on for all 1000 steps. Given
the assumptions, the rewards and probabilities of each possible chain of events can be
determined, and one need only pick the best. But the tree of possibilities grows extremely
rapidly; even if there were only two actions and two rewards, the tree would have 2 2000
leaves. It is generally not feasible to perform this immense computation exactly, but
perhaps it could be approximated efficiently. This approach would e↵ectively turn the
bandit problem into an instance of the full reinforcement learning problem. In the end, we
may be able to use approximate reinforcement learning methods such as those presented
in Part II of this book to approach this optimal solution. But that is a topic for research
and beyond the scope of this introductory book.
