
\begin{itemize}
    \item \emph{Stochastic}\label{def:stochastic} - randomly determined.
    \item \emph{Markov chain}\label{def:markov_chain} - stochastic model describing a sequence of
    states in which the probability of coming into any state is solely defained and
    attained from the previous state.
    \item \emph{Markov property}\label{def:markov_property} -
    a property of a process where he condtional probability distribution of
    future states is solely dependent on the current state: memorylessness.
    \item \emph{Markov process}\label{def:markov_process} -
    a stochastic process that satisfies the \myref{def:markov_property}{Markov property}.
    \item \emph{Markov decision process}\label{def:mdp} (MDP) -
    discrete time stochastic control process.
    Modeling decision making in situations where outcomes are partly random,
    partly in control of the decision maker.
    An MDP is essentially a \myref{def:markov_chain}{Markov chain} with added actions
    and rewards: interaction with an agent.
    \item \emph{Markov reward process}\label{def:mrp} (MRP) -
    an \myref{def:mdp}{MDP} without actions, the transitions are defined by a
    probability function.
\end{itemize}
