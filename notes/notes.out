\BOOKMARK [1][-]{section.0.1}{Definitions}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 2
\BOOKMARK [1][-]{section.1.1}{Basic Elements}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.2}{Summary}{chapter.1}% 4
\BOOKMARK [-1][-]{part.1}{I Tabular Solution Methods}{}% 5
\BOOKMARK [0][-]{chapter.2}{Multi-armed Bandits}{part.1}% 6
\BOOKMARK [1][-]{section.2.1}{k-armed Bandit Problem}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.2}{Action-value Methods}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.3}{The 10-armed Testbed}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.4}{Incremental Implementation}{chapter.2}% 10
\BOOKMARK [1][-]{section.2.5}{Tracking a Nonstationary Problem}{chapter.2}% 11
\BOOKMARK [1][-]{section.2.6}{Optimistic Initial Values}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.7}{Upper-Confidence-Bound \(UCB\) Action Selection}{chapter.2}% 13
\BOOKMARK [1][-]{section.2.8}{Gradient Bandit Algorithms}{chapter.2}% 14
\BOOKMARK [1][-]{section.2.9}{Associative Search \(Contextual Bandits\)}{chapter.2}% 15
\BOOKMARK [1][-]{section.2.10}{Summary}{chapter.2}% 16
\BOOKMARK [0][-]{chapter.3}{Finite Markov Decision Processes}{part.1}% 17
\BOOKMARK [1][-]{section.3.1}{The Agent-Environment Interface}{chapter.3}% 18
\BOOKMARK [1][-]{section.3.2}{Goals and Rewards}{chapter.3}% 19
\BOOKMARK [1][-]{section.3.3}{Returns and Episodes}{chapter.3}% 20
\BOOKMARK [1][-]{section.3.4}{Unified Notation for Episodic and Continuing Tasks}{chapter.3}% 21
\BOOKMARK [1][-]{section.3.5}{Policies and Value Functions}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.6}{Optimal Policies and Optimal Value Functions}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.7}{Optimality and Approximation}{chapter.3}% 24
\BOOKMARK [1][-]{section.3.8}{Summary}{chapter.3}% 25
\BOOKMARK [0][-]{chapter.4}{Dynamic Programming}{part.1}% 26
\BOOKMARK [1][-]{section.4.1}{Policy Evaluation \(Prediction\)}{chapter.4}% 27
\BOOKMARK [1][-]{section.4.2}{Policy Improvement}{chapter.4}% 28
\BOOKMARK [1][-]{section.4.3}{Policy Iteration}{chapter.4}% 29
\BOOKMARK [1][-]{section.4.4}{Value Iteration}{chapter.4}% 30
\BOOKMARK [1][-]{section.4.5}{Asynchronous Dynamic Programming}{chapter.4}% 31
\BOOKMARK [1][-]{section.4.6}{Generalized Policy Iteration \(GPI\)}{chapter.4}% 32
\BOOKMARK [1][-]{section.4.7}{Efficiency of Dynamic Programming}{chapter.4}% 33
\BOOKMARK [1][-]{section.4.8}{Summary}{chapter.4}% 34
\BOOKMARK [0][-]{chapter.5}{Monte Carlo \(MC\) Methods}{part.1}% 35
\BOOKMARK [1][-]{section.5.1}{Monte Carlo Prediction}{chapter.5}% 36
\BOOKMARK [1][-]{section.5.2}{Monte Carlo Estimation of Action Values}{chapter.5}% 37
\BOOKMARK [1][-]{section.5.3}{Monte Carlo Control}{chapter.5}% 38
\BOOKMARK [1][-]{section.5.4}{Monte Carlo Control without Exploring Starts}{chapter.5}% 39
\BOOKMARK [1][-]{section.5.5}{Off-policy Prediction via ImportanceSampling}{chapter.5}% 40
\BOOKMARK [1][-]{section.5.6}{Incremental Implementation}{chapter.5}% 41
\BOOKMARK [1][-]{section.5.7}{Off-policy Monte Carlo Control}{chapter.5}% 42
\BOOKMARK [1][-]{section.5.8}{*Discounting-aware Importance Sampling}{chapter.5}% 43
\BOOKMARK [1][-]{section.5.9}{*Per-decision Importance Sampling}{chapter.5}% 44
\BOOKMARK [1][-]{section.5.10}{Summary}{chapter.5}% 45
\BOOKMARK [0][-]{chapter.6}{Temporal-Difference Learning}{part.1}% 46
\BOOKMARK [1][-]{section.6.1}{TD Prediction}{chapter.6}% 47
\BOOKMARK [1][-]{section.6.2}{Advantages of TD Prediction Methods}{chapter.6}% 48
\BOOKMARK [1][-]{section.6.3}{Optimality of TD\(0\)}{chapter.6}% 49
\BOOKMARK [1][-]{section.6.4}{Sarsa: On-policy TD Control}{chapter.6}% 50
\BOOKMARK [1][-]{section.6.5}{Q-Learning: Off-policy TD Control}{chapter.6}% 51
\BOOKMARK [1][-]{section.6.6}{Expected SARSA}{chapter.6}% 52
\BOOKMARK [1][-]{section.6.7}{Maximization Bias and Double Learning}{chapter.6}% 53
\BOOKMARK [1][-]{section.6.8}{Games, Afterstates, and Other Special Cases}{chapter.6}% 54
\BOOKMARK [1][-]{section.6.9}{Summary}{chapter.6}% 55
\BOOKMARK [0][-]{chapter.7}{n-step Bootstrapping}{part.1}% 56
\BOOKMARK [1][-]{section.7.1}{n-step TD Prediction}{chapter.7}% 57
\BOOKMARK [1][-]{section.7.2}{n-step Sarsa}{chapter.7}% 58
\BOOKMARK [1][-]{section.7.3}{n-step Off-policy Learning}{chapter.7}% 59
\BOOKMARK [1][-]{section.7.4}{*Per-decision Methods with Control Variates}{chapter.7}% 60
\BOOKMARK [1][-]{section.7.5}{Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm}{chapter.7}% 61
\BOOKMARK [1][-]{section.7.6}{*A Unifying Algorithm: n-step Q\(\)}{chapter.7}% 62
\BOOKMARK [1][-]{section.7.7}{Summary}{chapter.7}% 63
\BOOKMARK [0][-]{chapter.8}{Planning and Learning with Tabular Methods}{part.1}% 64
\BOOKMARK [1][-]{section.8.1}{Models and Planning}{chapter.8}% 65
\BOOKMARK [1][-]{section.8.2}{Dyna: Integrated Planning, Acting, and Learning}{chapter.8}% 66
\BOOKMARK [1][-]{section.8.3}{When the Model Is Wrong}{chapter.8}% 67
\BOOKMARK [1][-]{section.8.4}{Prioritized Sweeping}{chapter.8}% 68
