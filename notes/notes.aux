\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Definitions}{4}{section.0.1}}
\newlabel{def:stochastic}{{0.1}{4}{Definitions}{section.0.1}{}}
\newlabel{def:markov_chain}{{0.1}{4}{Definitions}{section.0.1}{}}
\newlabel{def:markov_property}{{0.1}{4}{Definitions}{section.0.1}{}}
\newlabel{def:markov_process}{{0.1}{4}{Definitions}{section.0.1}{}}
\newlabel{def:mdp}{{0.1}{4}{Definitions}{section.0.1}{}}
\newlabel{def:mrp}{{0.1}{4}{Definitions}{section.0.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{5}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Basic Elements}{5}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Summary}{5}{section.1.2}}
\newlabel{sec:intro-summary}{{1.2}{5}{Summary}{section.1.2}{}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Tabular Solution Methods}{7}{part.1}}
\newlabel{part:tabular_solution_methods}{{I}{8}{Tabular Solution Methods}{part.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Multi-armed Bandits}{8}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:multi_armed_bandits}{{2}{8}{Multi-armed Bandits}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}k-armed Bandit Problem}{8}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Action-value Methods}{8}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The 10-armed Testbed}{8}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Incremental Implementation}{8}{section.2.4}}
\newlabel{eq:2.3}{{{2.3}}{8}{Incremental Implementation}{AMS.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Tracking a Nonstationary Problem}{9}{section.2.5}}
\newlabel{eq:2.5}{{{2.5}}{9}{Tracking a Nonstationary Problem}{AMS.3}{}}
\newlabel{eq:2.6}{{{2.6}}{9}{Tracking a Nonstationary Problem}{AMS.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimistic Initial Values}{9}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Upper-Confidence-Bound (UCB) Action Selection}{10}{section.2.7}}
\newlabel{eq:2.10}{{{2.10}}{10}{Upper-Confidence-Bound (UCB) Action Selection}{AMS.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Gradient Bandit Algorithms}{10}{section.2.8}}
\newlabel{eq:2.11}{{{2.11}}{10}{Gradient Bandit Algorithms}{AMS.6}{}}
\newlabel{eq:2.12}{{{2.12}}{10}{Gradient Bandit Algorithms}{AMS.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Associative Search (Contextual Bandits)}{11}{section.2.9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Summary}{11}{section.2.10}}
\newlabel{sec:mab-summary}{{2.10}{11}{Summary}{section.2.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Finite Markov Decision Processes}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:finite_mdps}{{3}{13}{Finite Markov Decision Processes}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Agent-Environment Interface}{13}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The agent-environment interaction in MPDs.}}{13}{figure.3.1}}
\newlabel{fig:3.1}{{3.1}{13}{The agent-environment interaction in MPDs}{figure.3.1}{}}
\newlabel{eq:3.2}{{{3.2}}{14}{The Agent-Environment Interface}{AMS.8}{}}
\newlabel{eq:3.3}{{{3.3}}{14}{The Agent-Environment Interface}{AMS.9}{}}
\newlabel{eq:3.4}{{{3.4}}{14}{The Agent-Environment Interface}{AMS.10}{}}
\newlabel{eq:3.5}{{{3.5}}{14}{The Agent-Environment Interface}{AMS.11}{}}
\newlabel{eq:3.6}{{{3.6}}{14}{The Agent-Environment Interface}{AMS.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Goals and Rewards}{14}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Returns and Episodes}{14}{section.3.3}}
\newlabel{eq:3.7}{{{3.7}}{14}{Returns and Episodes}{AMS.13}{}}
\newlabel{eq:3.8}{{{3.8}}{14}{Returns and Episodes}{AMS.14}{}}
\newlabel{eq:3.9}{{{3.9}}{15}{Returns and Episodes}{AMS.15}{}}
\newlabel{eq:3.10}{{{3.10}}{15}{Returns and Episodes}{AMS.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Unified Notation for Episodic and Continuing Tasks}{15}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Policies and Value Functions}{15}{section.3.5}}
\newlabel{eq:3.12}{{{3.12}}{15}{Policies and Value Functions}{AMS.17}{}}
\newlabel{eq:3.13}{{{3.13}}{16}{Policies and Value Functions}{AMS.18}{}}
\newlabel{eq:3.14}{{{3.14}}{16}{Policies and Value Functions}{AMS.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Gridworld example: exceptional reward dynamics (left) and state-value function for the equiprobable random policy (right). Suppose the agent selects all four actions (left, right, up, down) with equal probability in all states. $\gamma =0.9$.}}{17}{figure.3.2}}
\newlabel{fig:3.2}{{3.2}{17}{Gridworld example: exceptional reward dynamics (left) and state-value function for the equiprobable random policy (right). Suppose the agent selects all four actions (left, right, up, down) with equal probability in all states. $\gamma =0.9$}{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Optimal Policies and Optimal Value Functions}{19}{section.3.6}}
\newlabel{eq:3.60*}{{{3.60*}}{19}{Optimal Policies and Optimal Value Functions}{AMS.20}{}}
\newlabel{eq:3.15}{{{3.15}}{19}{Optimal Policies and Optimal Value Functions}{AMS.21}{}}
\newlabel{eq:3.16}{{{3.16}}{19}{Optimal Policies and Optimal Value Functions}{AMS.22}{}}
\newlabel{eq:3.17}{{{3.17}}{19}{Optimal Policies and Optimal Value Functions}{AMS.23}{}}
\newlabel{eq:3.18}{{{3.18}}{19}{Optimal Policies and Optimal Value Functions}{AMS.24}{}}
\newlabel{eq:3.20}{{{3.20}}{19}{Optimal Policies and Optimal Value Functions}{AMS.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Optimality and Approximation}{20}{section.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Summary}{20}{section.3.8}}
\newlabel{sec:fmdps-summary}{{3.8}{20}{Summary}{section.3.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Dynamic Programming}{23}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:dp}{{4}{23}{Dynamic Programming}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Policy Evaluation (Prediction)}{23}{section.4.1}}
\newlabel{sec:policy_evaluation}{{4.1}{23}{Policy Evaluation (Prediction)}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Policy Improvement}{23}{section.4.2}}
\newlabel{sec:policy_improvement}{{4.2}{23}{Policy Improvement}{section.4.2}{}}
\newlabel{eq:4.9}{{{4.9}}{24}{Policy Improvement}{AMS.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Policy Iteration}{24}{section.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Value Iteration}{24}{section.4.4}}
\newlabel{eq:4.10}{{{4.10}}{25}{Value Iteration}{AMS.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Asynchronous Dynamic Programming}{25}{section.4.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Generalized Policy Iteration (GPI)}{25}{section.4.6}}
\newlabel{sec:gpi}{{4.6}{25}{Generalized Policy Iteration (GPI)}{section.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Efficiency of Dynamic Programming}{26}{section.4.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Summary}{26}{section.4.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Monte Carlo (MC) Methods}{28}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:mcm}{{5}{28}{Monte Carlo (MC) Methods}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Monte Carlo Prediction}{28}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Monte Carlo Estimation of Action Values}{29}{section.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Monte Carlo Control}{29}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Monte Carlo Control without Exploring Starts}{29}{section.5.4}}
\newlabel{t:on_policy_methods}{{5.4}{29}{Monte Carlo Control without Exploring Starts}{section.5.4}{}}
\newlabel{t:off_policy_methods}{{5.4}{29}{Monte Carlo Control without Exploring Starts}{section.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Off-policy Prediction via Importance\newline  Sampling}{30}{section.5.5}}
\newlabel{sec:off_policy_prediction_via_importance_sampling}{{5.5}{30}{Off-policy Prediction via Importance\newline Sampling}{section.5.5}{}}
\newlabel{t:importance_sampling_ratio}{{5.5}{30}{Off-policy Prediction via Importance\newline Sampling}{section.5.5}{}}
\newlabel{eq:5.3}{{{5.3}}{30}{Off-policy Prediction via Importance\newline Sampling}{AMS.28}{}}
\newlabel{eq:5.4}{{{5.4}}{30}{Off-policy Prediction via Importance\newline Sampling}{AMS.29}{}}
\newlabel{eq:5.5}{{{5.5}}{31}{Off-policy Prediction via Importance\newline Sampling}{AMS.30}{}}
\newlabel{t:ordinary_importance_sampling}{{5.5}{31}{Off-policy Prediction via Importance\newline Sampling}{AMS.30}{}}
\newlabel{t:weighted_importance_sampling}{{5.5}{31}{Off-policy Prediction via Importance\newline Sampling}{AMS.30}{}}
\newlabel{eq:5.6}{{{5.6}}{31}{Off-policy Prediction via Importance\newline Sampling}{AMS.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Incremental Implementation}{31}{section.5.6}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Off-policy Monte Carlo Control}{32}{section.5.7}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}*Discounting-aware Importance Sampling}{32}{section.5.8}}
\newlabel{eq:5.9}{{{5.9}}{32}{*Discounting-aware Importance Sampling}{AMS.32}{}}
\newlabel{eq:5.10}{{{5.10}}{33}{*Discounting-aware Importance Sampling}{AMS.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}*Per-decision Importance Sampling}{33}{section.5.9}}
\newlabel{sec:per_decision_importance_sampling}{{5.9}{33}{*Per-decision Importance Sampling}{section.5.9}{}}
\newlabel{eq:5.15}{{{5.15}}{33}{*Per-decision Importance Sampling}{AMS.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Summary}{33}{section.5.10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Temporal-Difference Learning}{36}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:temporal_difference_learning}{{6}{36}{Temporal-Difference Learning}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}TD Prediction}{36}{section.6.1}}
\newlabel{sec:td_prediction}{{6.1}{36}{TD Prediction}{section.6.1}{}}
\newlabel{eq:6.2}{{{6.2}}{36}{TD Prediction}{AMS.35}{}}
\newlabel{t:tdl-one_step_td}{{6.1}{36}{TD Prediction}{AMS.35}{}}
\newlabel{t:td_error}{{6.1}{37}{TD Prediction}{AMS.35}{}}
\newlabel{eq:6.5}{{{6.5}}{37}{TD Prediction}{AMS.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Advantages of TD Prediction Methods}{37}{section.6.2}}
\newlabel{sec:advantages_of_td_prediction_methods}{{6.2}{37}{Advantages of TD Prediction Methods}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Optimality of TD(0)}{37}{section.6.3}}
\newlabel{sec:optimality_of_td0}{{6.3}{37}{Optimality of TD(0)}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Sarsa: On-policy TD Control}{38}{section.6.4}}
\newlabel{sec:sarsa_on_policy_td_control}{{6.4}{38}{Sarsa: On-policy TD Control}{section.6.4}{}}
\newlabel{eq:6.7}{{{6.7}}{38}{Sarsa: On-policy TD Control}{AMS.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Q-Learning: Off-policy TD Control}{38}{section.6.5}}
\newlabel{sec:q_learning_off_policy_td_control}{{6.5}{38}{Q-Learning: Off-policy TD Control}{section.6.5}{}}
\newlabel{eq:6.8}{{{6.8}}{38}{Q-Learning: Off-policy TD Control}{AMS.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Expected SARSA}{39}{section.6.6}}
\newlabel{sec:expected_sarsa}{{6.6}{39}{Expected SARSA}{section.6.6}{}}
\newlabel{eq:6.9}{{{6.9}}{39}{Expected SARSA}{AMS.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Maximization Bias and Double Learning}{39}{section.6.7}}
\newlabel{sec:maximization_bias_and_double_learning}{{6.7}{39}{Maximization Bias and Double Learning}{section.6.7}{}}
\newlabel{t:maximization_bias}{{6.7}{39}{Maximization Bias and Double Learning}{section.6.7}{}}
\newlabel{eq:6.10}{{{6.10}}{39}{Maximization Bias and Double Learning}{AMS.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Games, Afterstates, and Other Special Cases}{40}{section.6.8}}
\newlabel{sec:games_afterstates_and_other_special_cases}{{6.8}{40}{Games, Afterstates, and Other Special Cases}{section.6.8}{}}
\newlabel{t:afterstate}{{6.8}{40}{Games, Afterstates, and Other Special Cases}{section.6.8}{}}
\newlabel{t:afterstate_value_functions}{{6.8}{40}{Games, Afterstates, and Other Special Cases}{section.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Summary}{40}{section.6.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Two different state-action pairs that result in the same afterstate and therefore should have the same $Q$.}}{41}{figure.6.1}}
\newlabel{fig:afterstate_example}{{6.1}{41}{Two different state-action pairs that result in the same afterstate and therefore should have the same $Q$}{figure.6.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}$n$-step Bootstrapping}{43}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:n_step_bootstrapping}{{7}{43}{$n$-step Bootstrapping}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}$n$-step TD Prediction}{43}{section.7.1}}
\newlabel{sec:n_step_td_prediction}{{7.1}{43}{$n$-step TD Prediction}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The backup diagrams of $n$-step methods. These methods form a spectrum ranging from one-step TD methods to Monte Carlo methods.}}{43}{figure.7.1}}
\newlabel{fig:7.1}{{7.1}{43}{The backup diagrams of $n$-step methods. These methods form a spectrum ranging from one-step TD methods to Monte Carlo methods}{figure.7.1}{}}
\newlabel{eq:7.1}{{{7.1}}{44}{$n$-step TD Prediction}{AMS.41}{}}
\newlabel{eq:7.2}{{{7.2}}{44}{$n$-step TD Prediction}{AMS.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}$n$-step Sarsa}{44}{section.7.2}}
\newlabel{sec:n_step_sarsa}{{7.2}{44}{$n$-step Sarsa}{section.7.2}{}}
\newlabel{eq:7.5}{{{7.5}}{44}{$n$-step Sarsa}{AMS.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}$n$-step Off-policy Learning}{45}{section.7.3}}
\newlabel{sec:n_step_off_policy_learning}{{7.3}{45}{$n$-step Off-policy Learning}{section.7.3}{}}
\newlabel{eq:7.9}{{{7.9}}{45}{$n$-step Off-policy Learning}{AMS.44}{}}
\newlabel{eq:7.10}{{{7.10}}{45}{$n$-step Off-policy Learning}{AMS.45}{}}
\newlabel{eq:7.11}{{{7.11}}{45}{$n$-step Off-policy Learning}{AMS.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}*Per-decision Methods with Control Variates}{46}{section.7.4}}
\newlabel{sec:per_decision_methods_with_control_variates}{{7.4}{46}{*Per-decision Methods with Control Variates}{section.7.4}{}}
\newlabel{eq:7.13}{{{7.13}}{46}{*Per-decision Methods with Control Variates}{AMS.47}{}}
\newlabel{t:control_variate}{{7.4}{46}{*Per-decision Methods with Control Variates}{AMS.47}{}}
\newlabel{eq:7.14}{{{7.14}}{47}{*Per-decision Methods with Control Variates}{AMS.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{47}{section.7.5}}
\newlabel{sec:off_policy_learning_without_importance_sampling}{{7.5}{47}{Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{section.7.5}{}}
\newlabel{eq:7.15}{{{7.15}}{47}{Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{AMS.49}{}}
\newlabel{eq:7.16}{{{7.16}}{47}{Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{AMS.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}*A Unifying Algorithm: $n$-step $Q(\sigma )$}{49}{section.7.6}}
\newlabel{sec:a_unifying_algorithm_n_step_q_sigma}{{7.6}{49}{*A Unifying Algorithm: $n$-step $Q(\sigma )$}{section.7.6}{}}
\newlabel{eq:7.17}{{{7.17}}{49}{*A Unifying Algorithm: $n$-step $Q(\sigma )$}{AMS.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Summary}{50}{section.7.7}}
\newlabel{sec:nsb-summary}{{7.7}{50}{Summary}{section.7.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Planning and Learning with Tabular Methods}{52}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:planning_and_learning_with_tabular_methods}{{8}{52}{Planning and Learning with Tabular Methods}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Models and Planning}{52}{section.8.1}}
\newlabel{sec:models_and_planning}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\newlabel{t:distribution_model}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\newlabel{t:sample_model}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\newlabel{t:planning}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\newlabel{t:state_space_planning}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\newlabel{t:plan_space_planning}{{8.1}{52}{Models and Planning}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Dyna: Integrated Planning, Acting, and Learning}{53}{section.8.2}}
\newlabel{sec:dyna_integrated_planning_acting_and_learning}{{8.2}{53}{Dyna: Integrated Planning, Acting, and Learning}{section.8.2}{}}
\newlabel{t:model_learning}{{8.2}{53}{Dyna: Integrated Planning, Acting, and Learning}{section.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The general Dyna Architecture. Real experience, passing back and forth between the environment and the policy, affects policy and value functions in much the same way as does simulated experience generated by the model of the environment.}}{54}{figure.8.1}}
\newlabel{fig:dyna_architecture}{{8.1}{54}{The general Dyna Architecture. Real experience, passing back and forth between the environment and the policy, affects policy and value functions in much the same way as does simulated experience generated by the model of the environment}{figure.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}When the Model Is Wrong}{55}{section.8.3}}
\newlabel{sec:when_the_model_is_wrong}{{8.3}{55}{When the Model Is Wrong}{section.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Prioritized Sweeping}{55}{section.8.4}}
\newlabel{sec:prioritized_sweeping}{{8.4}{55}{Prioritized Sweeping}{section.8.4}{}}
